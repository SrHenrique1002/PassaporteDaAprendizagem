import streamlit as sl
import os
import time

# Importa√ß√µes LangChain e Google Gemini
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from dotenv import load_dotenv

# Garante que o diret√≥rio 'uploaded' exista e carrega vari√°veis de ambiente
os.makedirs("uploaded", exist_ok=True) 
load_dotenv()

# ==============================================================================
# 1. FUN√á√ïES DE CONFIGURA√á√ÉO DO LLM E RAG
# ==============================================================================

def load_prompt():
    """
    Carrega o template de prompt. A instru√ß√£o for√ßa o LLM a primeiro identificar os t√≥picos 
    da unidade (usando o contexto RAG) e depois gerar as perguntas.
    """
    prompt = """Voc√™ √© um analista pedag√≥gico e criador de pr√©-question√°rios de diagn√≥stico.
Sua miss√£o √© criar um Pr√©-Question√°rio de 5 perguntas (e suas respostas) para identificar a defasagem exata de um estudante.

# FASE 1: AN√ÅLISE DE CONTE√öDO
1. O usu√°rio forneceu a localiza√ß√£o curricular (Disciplina, Unidade, Ano) na 'Pergunta'.
2. Use o 'Contexto Curricular Detalhado' para **priorizar e listar** os principais t√≥picos encontrados que correspondem a essa localiza√ß√£o.

# FASE 2: GERA√á√ÉO DO PR√â-QUESTION√ÅRIO
1. Use **apenas** os t√≥picos listados na FASE 1 para criar 5 perguntas de m√∫ltipla escolha ou discursivas curtas.
2. O objetivo √© testar os diferentes assuntos abordados naquela Unidade.
3. Formate o resultado usando Markdown e inclua um cabe√ßalho claro, citando a Unidade/Bimestre focado.

Contexto Curricular Detalhado: {context}
Pergunta (Localiza√ß√£o da Defasagem): {question}

Se o contexto for insuficiente para identificar o conte√∫do daquela unidade, responda "O Curr√≠culo indexado n√£o detalha o conte√∫do da unidade solicitada. N√£o √© poss√≠vel gerar o question√°rio."
"""
    prompt = ChatPromptTemplate.from_template(prompt)
    return prompt

def load_llm():
    """Carrega e retorna o modelo de linguagem (LLM) do Gemini."""
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
    return llm

def format_docs(docs):
    """Formata os documentos recuperados em uma √∫nica string de contexto."""
    return "\n\n".join(doc.page_content for doc in docs)

# ==============================================================================
# 2. FUN√á√ïES DE PROCESSAMENTO DE DADOS (RAG e Limpeza)
# ==============================================================================

def extract_data(pdf_docs):
    """Carrega, divide PDFs e cria o VectorStore FAISS."""
    text_chunks = []
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size = 512,
        chunk_overlap = 30,
        length_function = len,
        separators= ["\n\n", "\n", ".", " "]
    )
    
    for pdf in pdf_docs:
        save_uploadedfile(pdf) 
        loader = PyPDFLoader(os.path.join('uploaded', pdf.name))
        text_chunks.extend(loader.load_and_split(text_splitter=text_splitter))
        
    if not text_chunks: return None 
        
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
    except Exception:
        sl.error("Erro ao carregar o modelo de embeddings. Verifique a chave de API.", icon="‚ùå")
        return None

    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)
    return vectorstore

def initialize_session_state():
    """Inicializa o estado de sess√£o da base de conhecimento."""
    if "knowledge_base" not in sl.session_state: sl.session_state["knowledge_base"] = None

def save_uploadedfile(uploadedfile):
    """Salva o arquivo PDF enviado na pasta 'uploaded'."""
    with open(os.path.join("uploaded", uploadedfile.name), "wb") as f: f.write(uploadedfile.getbuffer())

def remove_files():
    """Remove arquivos PDF do diret√≥rio 'uploaded'."""
    path = os.path.join(os.getcwd(), 'uploaded')
    for file_name in os.listdir(path):
        file = os.path.join(path, file_name)
        if os.path.isfile(file) and file.endswith(".pdf"): os.remove(file)

# ==============================================================================
# 3. INTERFACE STREAMLIT E L√ìGICA PRINCIPAL
# ==============================================================================

if __name__ == '__main__':
    
    initialize_session_state()
    
    sl.title("üî¨ Ferramenta de Triagem Curricular por LLM")
    
    # --- Sidebar (Upload e Processamento) ---
    with sl.sidebar:
        sl.markdown("## üìö Configura√ß√£o Curricular (RAG)")
        pdf_docs = sl.file_uploader(label="**1. Upload do(s) Curr√≠culo(s) em PDF:**", accept_multiple_files=True, type=["pdf"])
        submitted_pdf = sl.button("Indexar Curr√≠culo")
            
        if submitted_pdf:
            if pdf_docs:
                with sl.spinner("Processando Curr√≠culo (RAG Indexing)..."):
                    remove_files() 
                    sl.session_state.knowledge_base = extract_data(pdf_docs)
                if sl.session_state.knowledge_base:
                    sl.success("Curr√≠culo indexado com sucesso!", icon="‚úÖ")
                else:
                    sl.error("Erro na indexa√ß√£o. Verifique o PDF e a chave de API.", icon="‚ùå")
            
            time.sleep(3) 

    # --- Aplica√ß√£o Principal (Gera√ß√£o do Pr√©-Question√°rio) ---
    
    llm=load_llm()
    prompt=load_prompt()
    
    # Input principal: O usu√°rio insere a localiza√ß√£o da defasagem manualmente.
    query = sl.text_input(
        label='**2. Localiza√ß√£o da Defasagem:**', 
        placeholder="Ex: Matem√°tica, II Unidade, 5¬∫ ano (para buscar o conte√∫do program√°tico desse per√≠odo)."
    )
    
    if sl.session_state.get("knowledge_base") is None:
        # Mostra a mensagem de aviso caso o curr√≠culo n√£o tenha sido indexado
        sl.warning("Por favor, fa√ßa o Upload e Indexa√ß√£o do Curr√≠culo (Passo 1) para continuar.", icon="üö®")
    else:
        # Formul√°rio de Gera√ß√£o
        sl.success("Curr√≠culo pronto para consulta. Insira a localiza√ß√£o da defasagem abaixo.", icon="‚úÖ")
        
        if query:
            if sl.button("Gerar Question√°rio"):
                try:
                   
                    retriever = sl.session_state.knowledge_base.as_retriever(search_kwargs={"k": 2}) 
                    
                    # Cadeia RAG: Usa a QUERY (Localiza√ß√£o) para buscar no Contexto (PDF)
                    rag_chain = (
                        {"context": retriever | format_docs, "question": RunnablePassthrough()}
                        | prompt
                        | llm
                        | StrOutputParser()
                    )
                    
                    with sl.spinner("Buscando conte√∫do curricular e gerando question√°rio..."):
                        response = rag_chain.invoke(query)
                    
                    # Output
                    sl.subheader("3. Pr√©-Question√°rio de Diagn√≥stico Gerado:")
                    sl.markdown(response)
                    
                    sl.info(f"O question√°rio foi gerado com sucesso com base no conte√∫do da unidade solicitada: **{query}**.", icon="üí°")
                    
                except Exception as e:
                    print(f"Ocorreu um erro durante a execu√ß√£o do RAG: {e}")
                    sl.error("Ocorreu um erro ao processar sua solicita√ß√£o. Verifique sua chave de API ou se a busca falhou (tente formatar melhor o PDF).", icon="‚ùå")